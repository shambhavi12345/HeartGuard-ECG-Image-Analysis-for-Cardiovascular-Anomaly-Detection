{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af146d6c-9ea9-4db6-b827-28d6eb7cb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_15576\\549104939.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424447a3-533b-41b9-9629-587969591ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR     = r\"D:\\res_work\\ECG_analysis_for_CVD\\processed_images\"\n",
    "NUM_CLASSES  = 4\n",
    "BATCH_SIZE   = 4      # per-GPU batch\n",
    "ACCUM_STEPS  = 8      # effective batch = 32\n",
    "TOTAL_EPOCHS = 30\n",
    "LR_HEAD      = 1e-3\n",
    "LR_FEAT      = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TRAIN_RATIO  = 0.7\n",
    "TEST_RATIO   = 0.2\n",
    "VAL_RATIO    = 0.1\n",
    "PATIENCE     = 5      # early stopping patience\n",
    "DROP_PROB    = 0.5    # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3240673-dc61-4317-91a3-a41944bc1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomAffine(15, translate=(0.1,0.1), shear=10),\n",
    "    transforms.ColorJitter(0.2,0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.15), ratio=(0.3,3.3)),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "full_ds = datasets.ImageFolder(DATA_DIR)\n",
    "N = len(full_ds)\n",
    "n_train = int(TRAIN_RATIO * N)\n",
    "n_test  = int(TEST_RATIO  * N)\n",
    "n_val   = N - n_train - n_test\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "perm = torch.randperm(N, generator=g).tolist()\n",
    "train_idx = perm[:n_train]\n",
    "test_idx  = perm[n_train:n_train+n_test]\n",
    "val_idx   = perm[n_train+n_test:]\n",
    "\n",
    "# Balanced sampler for train\n",
    "train_targets  = [full_ds.targets[i] for i in train_idx]\n",
    "class_counts   = np.bincount(train_targets, minlength=NUM_CLASSES)\n",
    "class_weights  = 1. / class_counts\n",
    "sample_weights = [class_weights[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Subsets & Loaders\n",
    "train_ds = Subset(datasets.ImageFolder(DATA_DIR, transform=train_tf), train_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   val_idx)\n",
    "test_ds  = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,     num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,     num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c227d9a1-c3cb-4d80-bd4e-f9677593ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "for p in base_model.features[:16].parameters(): p.requires_grad = False\n",
    "for p in base_model.features[16:].parameters(): p.requires_grad = True\n",
    "\n",
    "in_features = base_model.classifier[0].in_features\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(DROP_PROB),\n",
    "    nn.Linear(in_features, 256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(DROP_PROB),\n",
    "    nn.Linear(256, NUM_CLASSES)\n",
    ")\n",
    "model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b5147f-7db9-455c-8af1-49555deb8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, targets, gamma=2.0, weight=None):\n",
    "    ce = nn.CrossEntropyLoss(weight=weight)\n",
    "    logp = -ce(logits, targets)\n",
    "    p = torch.exp(logp)\n",
    "    return ((1 - p) ** gamma * (-logp)).mean()\n",
    "\n",
    "criterion = focal_loss\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.features[16:].parameters(), 'lr': LR_FEAT},\n",
    "    {'params': model.classifier.parameters(),    'lr': LR_HEAD}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=PATIENCE):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_acc):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_acc\n",
    "        elif val_acc <= self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_acc\n",
    "            self.counter = 0\n",
    "\n",
    "early_stopper = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e316a1bb-b670-419b-a299-8c948302568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_15576\\2316480977.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_15576\\2316480977.py:19: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  running_loss += loss.item() * ACCUM_STEPS\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_15576\\2316480977.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train: loss=0.1819, acc=40.68% | Val: loss=0.1472, acc=40.43%\n",
      " → New best model saved!\n",
      "Epoch 2/30 | Train: loss=0.1235, acc=53.78% | Val: loss=0.1055, acc=60.64%\n",
      " → New best model saved!\n",
      "Epoch 3/30 | Train: loss=0.0870, acc=63.33% | Val: loss=0.0444, acc=79.79%\n",
      " → New best model saved!\n",
      "Epoch 4/30 | Train: loss=0.0708, acc=67.64% | Val: loss=0.0739, acc=63.83%\n",
      "Epoch 5/30 | Train: loss=0.0533, acc=75.19% | Val: loss=0.0124, acc=87.23%\n",
      " → New best model saved!\n",
      "Epoch 6/30 | Train: loss=0.0345, acc=83.20% | Val: loss=0.0174, acc=84.04%\n",
      "Epoch 7/30 | Train: loss=0.0281, acc=83.36% | Val: loss=0.0164, acc=88.30%\n",
      " → New best model saved!\n",
      "Epoch 8/30 | Train: loss=0.0245, acc=83.20% | Val: loss=0.0095, acc=92.55%\n",
      " → New best model saved!\n",
      "Epoch 9/30 | Train: loss=0.0180, acc=83.67% | Val: loss=0.0126, acc=88.30%\n",
      "Epoch 10/30 | Train: loss=0.0430, acc=79.04% | Val: loss=0.0244, acc=84.04%\n",
      "Epoch 11/30 | Train: loss=0.0256, acc=85.21% | Val: loss=0.0436, acc=79.79%\n",
      "Epoch 12/30 | Train: loss=0.0209, acc=87.21% | Val: loss=0.0061, acc=95.74%\n",
      " → New best model saved!\n",
      "Epoch 13/30 | Train: loss=0.0144, acc=89.68% | Val: loss=0.0054, acc=94.68%\n",
      "Epoch 14/30 | Train: loss=0.0100, acc=90.91% | Val: loss=0.0029, acc=96.81%\n",
      " → New best model saved!\n",
      "Epoch 15/30 | Train: loss=0.0194, acc=89.83% | Val: loss=0.0045, acc=95.74%\n",
      "Epoch 16/30 | Train: loss=0.0085, acc=92.91% | Val: loss=0.0067, acc=91.49%\n",
      "Epoch 17/30 | Train: loss=0.0096, acc=90.45% | Val: loss=0.0031, acc=93.62%\n",
      "Epoch 18/30 | Train: loss=0.0076, acc=91.37% | Val: loss=0.0068, acc=92.55%\n",
      "Epoch 19/30 | Train: loss=0.0077, acc=92.45% | Val: loss=0.0025, acc=97.87%\n",
      " → New best model saved!\n",
      "Epoch 20/30 | Train: loss=0.0106, acc=91.22% | Val: loss=0.0044, acc=95.74%\n",
      "Epoch 21/30 | Train: loss=0.0066, acc=93.22% | Val: loss=0.0028, acc=97.87%\n",
      "Epoch 22/30 | Train: loss=0.0077, acc=92.91% | Val: loss=0.0030, acc=97.87%\n",
      "Epoch 23/30 | Train: loss=0.0041, acc=95.84% | Val: loss=0.0022, acc=98.94%\n",
      " → New best model saved!\n",
      "Epoch 24/30 | Train: loss=0.0052, acc=94.30% | Val: loss=0.0013, acc=98.94%\n",
      "Epoch 25/30 | Train: loss=0.0066, acc=93.99% | Val: loss=0.0012, acc=98.94%\n",
      "Epoch 26/30 | Train: loss=0.0046, acc=93.68% | Val: loss=0.0016, acc=98.94%\n",
      "Epoch 27/30 | Train: loss=0.0033, acc=94.61% | Val: loss=0.0016, acc=98.94%\n",
      "Epoch 28/30 | Train: loss=0.0038, acc=94.92% | Val: loss=0.0030, acc=96.81%\n",
      "Early stopping at epoch 28\n",
      "Training complete. Best validation accuracy: 98.94%\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = running_corr = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        running_corr += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / n_train\n",
    "    train_acc  = running_corr / n_train * 100\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = val_corr = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            val_corr += (outputs.argmax(1) == labels).sum().item()\n",
    "    val_loss = val_loss / n_val\n",
    "    val_acc  = val_corr / n_val * 100\n",
    "\n",
    "    scheduler.step()\n",
    "    early_stopper(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_vgg16_(processed)ecg.pth\")\n",
    "        print(\" → New best model saved!\")\n",
    "    if early_stopper.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "print(f\"Training complete. Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ddbe9e-a27a-4ba3-8a3a-81d2575b8943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_15576\\2565363314.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.92%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_vgg16_(processed)ecg.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_corr = 0\n",
    "with torch.no_grad(), autocast():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        test_corr += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "test_acc = test_corr / n_test * 100\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcef12-4488-4221-b6a1-f868e3fdcb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
