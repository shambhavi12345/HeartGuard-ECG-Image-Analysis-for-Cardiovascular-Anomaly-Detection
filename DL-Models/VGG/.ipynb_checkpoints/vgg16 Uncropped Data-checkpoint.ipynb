{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df5bf4d6-1941-4e5b-ad15-698298dec3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0c768f2-0d01-4efb-8c4f-3029a8931a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Device & mixed-precision scaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 2) Hyperparameters\n",
    "DATA_DIR      = r\"C:\\Users\\sihus\\OneDrive\\Desktop\\MP DL\\processed_uncropped_images\"\n",
    "NUM_CLASSES   = 4\n",
    "BATCH_SIZE    = 4       # small per-GPU batch for VRAM headroom\n",
    "ACCUM_STEPS   = 8       # effective batch = 4 × 8 = 32\n",
    "TOTAL_EPOCHS  = 20\n",
    "LR_HEAD       = 1e-3    # head learning rate\n",
    "LR_FEAT       = 1e-4    # Conv4+5 learning rate\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "TRAIN_RATIO   = 0.7\n",
    "TEST_RATIO    = 0.2\n",
    "VAL_RATIO     = 0.1\n",
    "\n",
    "# 3) Transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), shear=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.15), ratio=(0.3,3.3)),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b5742c1-517f-49aa-9641-aceb61bb9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load dataset & split by index\n",
    "full_ds = datasets.ImageFolder(DATA_DIR)\n",
    "N = len(full_ds)\n",
    "n_train = int(TRAIN_RATIO * N)\n",
    "n_test  = int(TEST_RATIO  * N)\n",
    "n_val   = N - n_train - n_test\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "perm = torch.randperm(N, generator=g).tolist()\n",
    "train_idx = perm[:n_train]\n",
    "test_idx  = perm[n_train:n_train+n_test]\n",
    "val_idx   = perm[n_train+n_test:]\n",
    "\n",
    "# 5) Compute per-sample weights for Balanced Sampling\n",
    "train_targets = [full_ds.targets[i] for i in train_idx]\n",
    "class_counts  = np.bincount(train_targets, minlength=NUM_CLASSES)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# 6) Create Subsets with transforms\n",
    "train_ds = Subset(datasets.ImageFolder(DATA_DIR, transform=train_tf), train_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   val_idx)\n",
    "test_ds  = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          sampler=sampler, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22481f37-07c8-4929-8a01-84a1bf66d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Model setup: VGG-16, replace head, unfreeze Conv4+Conv5\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "# Freeze first three conv blocks (layers 0‐15)\n",
    "for p in model.features[:16].parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze Conv4 & Conv5 (layers 16 on)\n",
    "for p in model.features[16:].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Replace classifier\n",
    "in_f = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_f, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# 8) Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce    = nn.CrossEntropyLoss(weight=weight)\n",
    "    def forward(self, logits, targets):\n",
    "        logp = -self.ce(logits, targets)\n",
    "        p    = torch.exp(logp)\n",
    "        return -((1 - p) ** self.gamma) * logp\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0)\n",
    "\n",
    "# 9) Optimizer & Cosine Annealing LR\n",
    "# Two param-groups: conv features + head\n",
    "opt = optim.AdamW([\n",
    "    {'params': model.features[16:].parameters(), 'lr': LR_FEAT},\n",
    "    {'params': model.classifier.parameters(),    'lr': LR_HEAD}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=TOTAL_EPOCHS, eta_min=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cdaac76-59e8-48a8-a180-62d4fd098317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train: loss=0.2675, acc=29.43% |  Val: loss=0.1254, acc=50.00%\n",
      "  → New best model saved!\n",
      "Epoch 2/20 | Train: loss=0.1145, acc=58.71% |  Val: loss=0.0742, acc=63.83%\n",
      "  → New best model saved!\n",
      "Epoch 3/20 | Train: loss=0.0721, acc=68.88% |  Val: loss=0.0963, acc=64.89%\n",
      "  → New best model saved!\n",
      "Epoch 4/20 | Train: loss=0.0563, acc=77.04% |  Val: loss=0.0242, acc=89.36%\n",
      "  → New best model saved!\n",
      "Epoch 5/20 | Train: loss=0.0479, acc=77.50% |  Val: loss=0.0105, acc=88.30%\n",
      "Epoch 6/20 | Train: loss=0.0287, acc=84.59% |  Val: loss=0.0359, acc=84.04%\n",
      "Epoch 7/20 | Train: loss=0.0369, acc=82.74% |  Val: loss=0.0263, acc=85.11%\n",
      "Epoch 8/20 | Train: loss=0.0237, acc=87.52% |  Val: loss=0.0279, acc=84.04%\n",
      "Epoch 9/20 | Train: loss=0.0235, acc=85.52% |  Val: loss=0.0115, acc=87.23%\n",
      "Epoch 10/20 | Train: loss=0.0217, acc=86.44% |  Val: loss=0.0124, acc=92.55%\n",
      "  → New best model saved!\n",
      "Epoch 11/20 | Train: loss=0.0155, acc=88.14% |  Val: loss=0.0085, acc=92.55%\n",
      "Epoch 12/20 | Train: loss=0.0104, acc=91.83% |  Val: loss=0.0060, acc=94.68%\n",
      "  → New best model saved!\n",
      "Epoch 13/20 | Train: loss=0.0132, acc=89.68% |  Val: loss=0.0019, acc=97.87%\n",
      "  → New best model saved!\n",
      "Epoch 14/20 | Train: loss=0.0063, acc=91.99% |  Val: loss=0.0010, acc=98.94%\n",
      "  → New best model saved!\n",
      "Epoch 15/20 | Train: loss=0.0071, acc=91.06% |  Val: loss=0.0163, acc=90.43%\n",
      "Epoch 16/20 | Train: loss=0.0073, acc=91.37% |  Val: loss=0.0013, acc=98.94%\n",
      "Epoch 17/20 | Train: loss=0.0053, acc=93.68% |  Val: loss=0.0034, acc=97.87%\n",
      "Epoch 18/20 | Train: loss=0.0065, acc=94.92% |  Val: loss=0.0024, acc=97.87%\n",
      "Epoch 19/20 | Train: loss=0.0061, acc=92.91% |  Val: loss=0.0019, acc=97.87%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m running_corr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, lbls) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     12\u001b[0m     imgs, lbls \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), lbls\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 10) Training loop with mixed precision & grad accumulation\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corr = 0\n",
    "    opt.zero_grad()\n",
    "\n",
    "    for i, (imgs, lbls) in enumerate(train_loader):\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        with autocast():\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, lbls) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        running_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    train_loss = running_loss / n_train\n",
    "    train_acc  = running_corr / n_train * 100\n",
    "\n",
    "    # — Validate —\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corr = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model(imgs)\n",
    "            val_loss += criterion(logits, lbls).item()\n",
    "            val_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    val_loss = val_loss / n_val\n",
    "    val_acc  = val_corr / n_val * 100\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "          f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "          f\" Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_vgg16_ecg_uncropped.pth\")\n",
    "        print(\"  → New best model saved!\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54706e-ce87-40bb-a7ba-f62423a4ce78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a038cc61-d566-4cfe-b981-277d82922e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to ECG image:  C:\\Users\\FireFly\\Desktop\\MI_100.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class: Myocardial Infarction\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Configuration\n",
    "# -------------------------------\n",
    "class_names = [\n",
    "    \"Myocardial Infarction\",\n",
    "    \"Abnormal Heartbeat\",\n",
    "    \"History of MI\",\n",
    "    \"Normal\"\n",
    "]\n",
    "\n",
    "checkpoint_path = r\"D:\\res_work\\ECG_analysis_for_CVD\\best_vgg16_ecg.pth\"\n",
    "num_classes     = len(class_names)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Transforms (same as training)\n",
    "# -------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Model\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = vgg.classifier[6].in_features\n",
    "vgg.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "vgg.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "vgg = vgg.to(device)\n",
    "vgg.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ecd60-f244-4e63-86be-257a3c6986ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Inference Function\n",
    "# -------------------------------\n",
    "def classify_ecg(image_path: str) -> str:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inp = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vgg(inp)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[pred.item()]\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Run from CLI\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    path = input(\"Enter path to ECG image: \").strip()\n",
    "    try:\n",
    "        label = classify_ecg(path)\n",
    "        print(f\"\\nPredicted class: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
