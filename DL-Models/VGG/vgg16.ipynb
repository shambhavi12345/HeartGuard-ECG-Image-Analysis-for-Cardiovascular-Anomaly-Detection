{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5bf4d6-1941-4e5b-ad15-698298dec3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_11676\\3147357983.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_11676\\3147357983.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_11676\\3147357983.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train: loss=0.2677, acc=27.85% |  Val: loss=0.1815, acc=28.72%\n",
      "  → New best model saved!\n",
      "Epoch 2/30 | Train: loss=0.1552, acc=43.23% |  Val: loss=0.1105, acc=60.64%\n",
      "  → New best model saved!\n",
      "Epoch 3/30 | Train: loss=0.1190, acc=58.31% |  Val: loss=0.3281, acc=47.87%\n",
      "Epoch 4/30 | Train: loss=0.1040, acc=62.00% |  Val: loss=0.0428, acc=82.98%\n",
      "  → New best model saved!\n",
      "Epoch 5/30 | Train: loss=0.0865, acc=66.92% |  Val: loss=0.0912, acc=62.77%\n",
      "Epoch 6/30 | Train: loss=0.0800, acc=66.46% |  Val: loss=0.0813, acc=77.66%\n",
      "Epoch 7/30 | Train: loss=0.0710, acc=70.46% |  Val: loss=0.0542, acc=82.98%\n",
      "Epoch 8/30 | Train: loss=0.0654, acc=73.08% |  Val: loss=0.0823, acc=75.53%\n",
      "Epoch 9/30 | Train: loss=0.0638, acc=68.31% |  Val: loss=0.0586, acc=79.79%\n",
      "Epoch 10/30 | Train: loss=0.0362, acc=78.46% |  Val: loss=0.0609, acc=88.30%\n",
      "  → New best model saved!\n",
      "Epoch 11/30 | Train: loss=0.0337, acc=80.46% |  Val: loss=0.0756, acc=82.98%\n",
      "Epoch 12/30 | Train: loss=0.0287, acc=81.08% |  Val: loss=0.1476, acc=71.28%\n",
      "Epoch 13/30 | Train: loss=0.0299, acc=83.54% |  Val: loss=0.0416, acc=91.49%\n",
      "  → New best model saved!\n",
      "Epoch 14/30 | Train: loss=0.0154, acc=88.62% |  Val: loss=0.0467, acc=93.62%\n",
      "  → New best model saved!\n",
      "Epoch 15/30 | Train: loss=0.0259, acc=86.46% |  Val: loss=0.0625, acc=87.23%\n",
      "Epoch 16/30 | Train: loss=0.0172, acc=88.77% |  Val: loss=0.0398, acc=90.43%\n",
      "Epoch 17/30 | Train: loss=0.0141, acc=88.31% |  Val: loss=0.0492, acc=91.49%\n",
      "Epoch 18/30 | Train: loss=0.0109, acc=91.54% |  Val: loss=0.0534, acc=94.68%\n",
      "  → New best model saved!\n",
      "Epoch 19/30 | Train: loss=0.0094, acc=91.23% |  Val: loss=0.0517, acc=89.36%\n",
      "Epoch 20/30 | Train: loss=0.0070, acc=92.15% |  Val: loss=0.0465, acc=92.55%\n",
      "Epoch 21/30 | Train: loss=0.0090, acc=91.23% |  Val: loss=0.0497, acc=92.55%\n",
      "Epoch 22/30 | Train: loss=0.0091, acc=92.46% |  Val: loss=0.0431, acc=93.62%\n",
      "Epoch 23/30 | Train: loss=0.0058, acc=93.23% |  Val: loss=0.0435, acc=93.62%\n",
      "Epoch 24/30 | Train: loss=0.0058, acc=94.00% |  Val: loss=0.0465, acc=94.68%\n",
      "Epoch 25/30 | Train: loss=0.0082, acc=91.08% |  Val: loss=0.0513, acc=91.49%\n",
      "Epoch 26/30 | Train: loss=0.0069, acc=92.62% |  Val: loss=0.0496, acc=94.68%\n",
      "Epoch 27/30 | Train: loss=0.0041, acc=94.62% |  Val: loss=0.0501, acc=94.68%\n",
      "Epoch 28/30 | Train: loss=0.0086, acc=91.23% |  Val: loss=0.0514, acc=93.62%\n",
      "Epoch 29/30 | Train: loss=0.0078, acc=92.77% |  Val: loss=0.0531, acc=92.55%\n",
      "Epoch 30/30 | Train: loss=0.0057, acc=93.38% |  Val: loss=0.0530, acc=92.55%\n",
      "\n",
      "Training complete. Best validation accuracy: 94.68%\n"
     ]
    }
   ],
   "source": [
    "# train_vgg16_ecg_improved.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# 1) Device & mixed-precision scaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 2) Hyperparameters\n",
    "DATA_DIR      = r\"D:\\res_work\\ECG_analysis_for_CVD\\PCA\\extrct\"\n",
    "NUM_CLASSES   = 4\n",
    "BATCH_SIZE    = 4       # small per-GPU batch for VRAM headroom\n",
    "ACCUM_STEPS   = 8       # effective batch = 4 × 8 = 32\n",
    "TOTAL_EPOCHS  = 30\n",
    "LR_HEAD       = 1e-3    # head learning rate\n",
    "LR_FEAT       = 1e-4    # Conv4+5 learning rate\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "TRAIN_RATIO   = 0.7\n",
    "TEST_RATIO    = 0.2\n",
    "VAL_RATIO     = 0.1\n",
    "\n",
    "# 3) Transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), shear=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.15), ratio=(0.3,3.3)),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 4) Load dataset & split by index\n",
    "full_ds = datasets.ImageFolder(DATA_DIR)\n",
    "N = len(full_ds)\n",
    "n_train = int(TRAIN_RATIO * N)\n",
    "n_test  = int(TEST_RATIO  * N)\n",
    "n_val   = N - n_train - n_test\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "perm = torch.randperm(N, generator=g).tolist()\n",
    "train_idx = perm[:n_train]\n",
    "test_idx  = perm[n_train:n_train+n_test]\n",
    "val_idx   = perm[n_train+n_test:]\n",
    "\n",
    "# 5) Compute per-sample weights for Balanced Sampling\n",
    "train_targets = [full_ds.targets[i] for i in train_idx]\n",
    "class_counts  = np.bincount(train_targets, minlength=NUM_CLASSES)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# 6) Create Subsets with transforms\n",
    "train_ds = Subset(datasets.ImageFolder(DATA_DIR, transform=train_tf), train_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   val_idx)\n",
    "test_ds  = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          sampler=sampler, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)\n",
    "\n",
    "# 7) Model setup: VGG-16, replace head, unfreeze Conv4+Conv5\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "# Freeze first three conv blocks (layers 0‐15)\n",
    "for p in model.features[:16].parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze Conv4 & Conv5 (layers 16 on)\n",
    "for p in model.features[16:].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Replace classifier\n",
    "in_f = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_f, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# 8) Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce    = nn.CrossEntropyLoss(weight=weight)\n",
    "    def forward(self, logits, targets):\n",
    "        logp = -self.ce(logits, targets)\n",
    "        p    = torch.exp(logp)\n",
    "        return -((1 - p) ** self.gamma) * logp\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0)\n",
    "\n",
    "# 9) Optimizer & Cosine Annealing LR\n",
    "# Two param-groups: conv features + head\n",
    "opt = optim.AdamW([\n",
    "    {'params': model.features[16:].parameters(), 'lr': LR_FEAT},\n",
    "    {'params': model.classifier.parameters(),    'lr': LR_HEAD}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=TOTAL_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# 10) Training loop with mixed precision & grad accumulation\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corr = 0\n",
    "    opt.zero_grad()\n",
    "\n",
    "    for i, (imgs, lbls) in enumerate(train_loader):\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        with autocast():\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, lbls) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        running_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    train_loss = running_loss / n_train\n",
    "    train_acc  = running_corr / n_train * 100\n",
    "\n",
    "    # — Validate —\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corr = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model(imgs)\n",
    "            val_loss += criterion(logits, lbls).item()\n",
    "            val_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    val_loss = val_loss / n_val\n",
    "    val_acc  = val_corr / n_val * 100\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "          f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "          f\" Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_vgg16_ecg.pth\")\n",
    "        print(\"  → New best model saved!\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e361e0f9-2661-41da-8ba8-50754ba36822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0.dev20250429+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)            # Should show something like 2.8.0.dev20250429+cu118\n",
    "print(torch.cuda.is_available())    # Should be True\n",
    "print(torch.cuda.get_device_name(0))# Should report your RTX 3050 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a038cc61-d566-4cfe-b981-277d82922e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter path to ECG image:  C:\\Users\\FireFly\\Desktop\\MI_100.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class: Myocardial Infarction\n"
     ]
    }
   ],
   "source": [
    "# infer_vgg16_ecg.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Configuration\n",
    "# -------------------------------\n",
    "class_names = [\n",
    "    \"Myocardial Infarction\",\n",
    "    \"Abnormal Heartbeat\",\n",
    "    \"History of MI\",\n",
    "    \"Normal\"\n",
    "]\n",
    "\n",
    "checkpoint_path = r\"D:\\res_work\\ECG_analysis_for_CVD\\best_vgg16_ecg.pth\"\n",
    "num_classes     = len(class_names)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Transforms (same as training)\n",
    "# -------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load Model\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "in_features = vgg.classifier[6].in_features\n",
    "vgg.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "vgg.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "vgg = vgg.to(device)\n",
    "vgg.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Inference Function\n",
    "# -------------------------------\n",
    "def classify_ecg(image_path: str) -> str:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    inp = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = vgg(inp)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[pred.item()]\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Run from CLI\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    path = input(\"Enter path to ECG image: \").strip()\n",
    "    try:\n",
    "        label = classify_ecg(path)\n",
    "        print(f\"\\nPredicted class: {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ecd60-f244-4e63-86be-257a3c6986ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_18852\\1246743983.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_18852\\1246743983.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_18852\\1246743983.py:136: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  running_loss += loss.item() * ACCUM_STEPS\n",
      "C:\\Users\\FireFly\\AppData\\Local\\Temp\\ipykernel_18852\\1246743983.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train: loss=0.2067, acc=37.90% |  Val: loss=0.1423, acc=44.68%\n",
      "  → New best model saved!\n"
     ]
    }
   ],
   "source": [
    "# train_vgg16_ecg_improved.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# 1) Device & mixed-precision scaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 2) Hyperparameters\n",
    "DATA_DIR      = r\"D:\\res_work\\ECG_analysis_for_CVD\\processed_images\"\n",
    "NUM_CLASSES   = 4\n",
    "BATCH_SIZE    = 4       # small per-GPU batch for VRAM headroom\n",
    "ACCUM_STEPS   = 8       # effective batch = 4 × 8 = 32\n",
    "TOTAL_EPOCHS  = 30\n",
    "LR_HEAD       = 1e-3    # head learning rate\n",
    "LR_FEAT       = 1e-4    # Conv4+5 learning rate\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "TRAIN_RATIO   = 0.7\n",
    "TEST_RATIO    = 0.2\n",
    "VAL_RATIO     = 0.1\n",
    "\n",
    "# 3) Transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1,0.1), shear=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02,0.15), ratio=(0.3,3.3)),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 4) Load dataset & split by index\n",
    "full_ds = datasets.ImageFolder(DATA_DIR)\n",
    "N = len(full_ds)\n",
    "n_train = int(TRAIN_RATIO * N)\n",
    "n_test  = int(TEST_RATIO  * N)\n",
    "n_val   = N - n_train - n_test\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "perm = torch.randperm(N, generator=g).tolist()\n",
    "train_idx = perm[:n_train]\n",
    "test_idx  = perm[n_train:n_train+n_test]\n",
    "val_idx   = perm[n_train+n_test:]\n",
    "\n",
    "# 5) Compute per-sample weights for Balanced Sampling\n",
    "train_targets = [full_ds.targets[i] for i in train_idx]\n",
    "class_counts  = np.bincount(train_targets, minlength=NUM_CLASSES)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[t] for t in train_targets]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# 6) Create Subsets with transforms\n",
    "train_ds = Subset(datasets.ImageFolder(DATA_DIR, transform=train_tf), train_idx)\n",
    "val_ds   = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   val_idx)\n",
    "test_ds  = Subset(datasets.ImageFolder(DATA_DIR, transform=val_tf),   test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                          sampler=sampler, num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
    "                          shuffle=False, num_workers=4)\n",
    "\n",
    "# 7) Model setup: VGG-16, replace head, unfreeze Conv4+Conv5\n",
    "model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "# Freeze first three conv blocks (layers 0‐15)\n",
    "for p in model.features[:16].parameters():\n",
    "    p.requires_grad = False\n",
    "# Unfreeze Conv4 & Conv5 (layers 16 on)\n",
    "for p in model.features[16:].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Replace classifier\n",
    "in_f = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_f, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# 8) Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce    = nn.CrossEntropyLoss(weight=weight)\n",
    "    def forward(self, logits, targets):\n",
    "        logp = -self.ce(logits, targets)\n",
    "        p    = torch.exp(logp)\n",
    "        return -((1 - p) ** self.gamma) * logp\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0)\n",
    "\n",
    "# 9) Optimizer & Cosine Annealing LR\n",
    "# Two param-groups: conv features + head\n",
    "opt = optim.AdamW([\n",
    "    {'params': model.features[16:].parameters(), 'lr': LR_FEAT},\n",
    "    {'params': model.classifier.parameters(),    'lr': LR_HEAD}\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=TOTAL_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "# 10) Training loop with mixed precision & grad accumulation\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    # — Train —\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corr = 0\n",
    "    opt.zero_grad()\n",
    "\n",
    "    for i, (imgs, lbls) in enumerate(train_loader):\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        with autocast():\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, lbls) / ACCUM_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % ACCUM_STEPS == 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        running_loss += loss.item() * ACCUM_STEPS\n",
    "        running_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    train_loss = running_loss / n_train\n",
    "    train_acc  = running_corr / n_train * 100\n",
    "\n",
    "    # — Validate —\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corr = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model(imgs)\n",
    "            val_loss += criterion(logits, lbls).item()\n",
    "            val_corr += (logits.argmax(1) == lbls).sum().item()\n",
    "\n",
    "    val_loss = val_loss / n_val\n",
    "    val_acc  = val_corr / n_val * 100\n",
    "\n",
    "    sched.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS} | \"\n",
    "          f\"Train: loss={train_loss:.4f}, acc={train_acc:.2f}% | \"\n",
    "          f\" Val: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_vgg16_(processed)ecg.pth\")\n",
    "        print(\"  → New best model saved!\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c4c78-cf74-4c48-b2cb-d32640b3561f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
